<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <meta name="description" content="Segmentation Techniques in Image Processing">
    <meta name="author" content="A. Giuliano Mirabella">
    <title>Segmentation</title>
    <link rel="stylesheet" href="styles.css">
</head>
<body>
    <header>
        <h1>Segmentation</h1>
        <p>Understanding image segmentation methods and their broad applications</p>
    </header>

    <div class="container">
        <!-- 1. WHAT IS IMAGE SEGMENTATION? -->
        <h2>What is Image Segmentation?</h2>
        <p>
            <strong>Image segmentation</strong> is the process of partitioning an image into coherent, meaningful regions. 
            In essence, segmentation algorithms seek to simplify or change the representation of an image to make it more understandable and easier to analyze. 
            Rather than considering every pixel individually, segmentation allows us to group pixels that share certain characteristics (e.g., intensity, color, texture) 
            into larger regions that may correspond to objects, organs, or areas of interest.
        </p>
        <p>
            Segmentation is a cornerstone of many computer vision and image processing pipelines—particularly in <em>medical imaging</em>, 
            <em>autonomous systems</em>, and <em>remote sensing</em>. By isolating structures like tumors, roads, or defects, we can perform higher-level tasks 
            such as object recognition, volumetric measurement, or anomaly detection.
        </p>

        <!-- 2. WHY IS SEGMENTATION IMPORTANT? -->
        <h2>Why is Segmentation Important?</h2>
        <p>
            Segmentation drastically reduces the complexity of subsequent image analysis steps and forms the backbone of many critical applications:
        </p>
        <ul>
            <li>
                <strong>Object Detection:</strong><br>
                Precise segmentation helps in accurately localizing and classifying objects within an image or video, 
                which is vital for surveillance, robotics, and industrial inspection.
            </li>
            <li>
                <strong>Medical Diagnosis:</strong><br>
                By delineating structures such as organs or lesions in medical scans (CT, MRI, Ultrasound), segmentation aids in treatment planning, 
                disease monitoring, and surgical guidance.
            </li>
            <li>
                <strong>Autonomous Systems:</strong><br>
                Self-driving cars, drones, and service robots rely heavily on robust segmentation to understand road layouts, detect pedestrians, and navigate safely.
            </li>
            <li>
                <strong>Quality Control:</strong><br>
                Automated inspection lines in manufacturing utilize segmentation to highlight defective items, chips, or irregularities.
            </li>
        </ul>

        <!-- 3. TYPES OF IMAGE SEGMENTATION TECHNIQUES -->
        <h2>Types of Image Segmentation Techniques</h2>
        <p>
            Segmentation techniques can be broadly classified according to the core principle or algorithmic approach they employ. 
            Below are several common categories:
        </p>

        <!-- 3.1 THRESHOLDING-BASED SEGMENTATION -->
        <h3>1. Thresholding-Based Segmentation</h3>
        <p>
            <strong>Thresholding</strong> is one of the simplest yet most widely used segmentation strategies. 
            It converts an image from grayscale (or color) to a binary image, differentiating foreground (objects) from the background 
            by setting a cut-off intensity value (the threshold).
        </p>
        <ul>
            <li>
                <strong>Global Thresholding:</strong><br>
                Uses a single threshold value for the entire image, typically determined by analyzing the overall histogram. 
                Most effective when the foreground and background intensities are well-separated.
            </li>
            <li>
                <strong>Adaptive (Local) Thresholding:</strong><br>
                Computes threshold values for small sub-regions in the image. 
                Useful when illumination or contrast varies across different parts of the image.
            </li>
            <li>
                <strong>Otsu’s Method:</strong><br>
                An automatic approach that selects an “optimal” global threshold by minimizing the within-class variance of the two segmented groups. 
                Often used as a robust default method in applications where no prior knowledge of threshold is available.
            </li>
        </ul>

        <!-- Code Tabs: Python and MATLAB (Otsu) -->
        <div class="code-tabs">
            <div class="code-tab active" onclick="switchCode('python-otsu')">Python</div>
            <div class="code-tab" onclick="switchCode('matlab-otsu')">MATLAB</div>
        </div>

        <div id="python-otsu" class="code-block active">
            <pre>
import cv2

# Load grayscale image
image = cv2.imread('image.jpg', cv2.IMREAD_GRAYSCALE)

# Apply Otsu's thresholding
_, thresholded = cv2.threshold(image, 0, 255, cv2.THRESH_BINARY + cv2.THRESH_OTSU)
cv2.imwrite('thresholded.jpg', thresholded)
            </pre>
        </div>

        <div id="matlab-otsu" class="code-block">
            <pre>
% Load grayscale image
image = imread('image.jpg');
grayImage = rgb2gray(image);  % Convert to grayscale if needed

% Apply Otsu's thresholding
level = graythresh(grayImage);      % Compute Otsu's threshold (normalized 0-1)
thresholded = imbinarize(grayImage, level);  % Apply threshold

% Convert logical to uint8 for saving (0 or 255)
thresholded = uint8(thresholded) * 255;
imwrite(thresholded, 'thresholded.jpg');
            </pre>
        </div>

        <!-- 3.2 EDGE-BASED SEGMENTATION -->
        <h3>2. Edge-Based Segmentation</h3>
        <p>
            <strong>Edge-based</strong> methods locate objects by identifying discontinuities or sharp intensity transitions in an image. 
            Edges often mark the boundaries between distinct objects or regions.
        </p>
        <ul>
            <li>
                <strong>Sobel Operator:</strong><br>
                Computes intensity gradients in the horizontal and vertical directions. 
                The resulting gradient magnitude highlights edges.
            </li>
            <li>
                <strong>Canny Edge Detector:</strong><br>
                A multi-stage algorithm that includes noise reduction, gradient calculation, non-maximum suppression, 
                double-thresholding, and edge tracking by hysteresis. 
                Considered one of the more robust classical edge detection methods.
            </li>
        </ul>

        <!-- Code Tabs: Python and MATLAB (Canny) -->
        <div class="code-tabs">
            <div class="code-tab active" onclick="switchCode('python-canny')">Python</div>
            <div class="code-tab" onclick="switchCode('matlab-canny')">MATLAB</div>
        </div>

        <div id="python-canny" class="code-block active">
            <pre>
# Canny Edge Detection
edges = cv2.Canny(image, 50, 150)
cv2.imwrite('edges.jpg', edges)
            </pre>
        </div>

        <div id="matlab-canny" class="code-block">
            <pre>
% Apply Canny edge detection
edges = edge(grayImage, 'Canny', [50 150]/255);

% Convert logical result to uint8 (0 or 255) for saving
edges = uint8(edges) * 255;
imwrite(edges, 'edges.jpg');
            </pre>
        </div>

        <!-- 3.3 REGION-BASED SEGMENTATION -->
        <h3>3. Region-Based Segmentation</h3>
        <p>
            <strong>Region-based</strong> methods group neighboring pixels into regions based on similarity criteria (e.g., intensity, texture, color). 
            Unlike edge-based approaches, which focus on boundaries, region-based segmentation attempts to determine the homogeneous areas directly.
        </p>
        <ul>
            <li>
                <strong>Region Growing:</strong><br>
                Starts from one or more “seed” pixels and expands outward as long as neighboring pixels fulfill a similarity condition (like intensity difference). 
                This simple approach can be effective if seeds and criteria are well chosen.
            </li>
            <li>
                <strong>Watershed Algorithm:</strong><br>
                Conceptualizes the grayscale image as a topographic surface. 
                Regions “flood” from the minima, and barriers (watershed lines) form where floods from different minima meet. 
                Particularly effective in scenarios where clear intensity ridges separate distinct objects.
            </li>
        </ul>

        <!-- Code Tabs: Python and MATLAB (Simple Watershed Example) -->
        <div class="code-tabs">
            <div class="code-tab active" onclick="switchCode('python-water')">Python</div>
            <div class="code-tab" onclick="switchCode('matlab-water')">MATLAB</div>
        </div>

        <div id="python-water" class="code-block active">
            <pre>
# Watershed Algorithm (simplified illustration)
import cv2

# Assume 'image' is a loaded BGR image
gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)
# Basic threshold for foreground detection
_, binary = cv2.threshold(gray, 0, 255, cv2.THRESH_BINARY_INV + cv2.THRESH_OTSU)

# Save binary result (in practice, additional steps needed for watershed)
cv2.imwrite('watershed_result.jpg', binary)
            </pre>
        </div>

        <div id="matlab-water" class="code-block">
            <pre>
% Apply Otsu's thresholding with inverse binary mode
level = graythresh(grayImage);
binary = imbinarize(grayImage, level);
binary = ~binary;  % invert binary (like THRESH_BINARY_INV)

% Save result
binary_uint8 = uint8(binary) * 255;
imwrite(binary_uint8, 'watershed_result.jpg');
            </pre>
        </div>

        <!-- 3.4 CLUSTERING-BASED SEGMENTATION -->
        <h3>4. Clustering-Based Segmentation</h3>
        <p>
            <strong>Clustering-based</strong> segmentation is an unsupervised learning approach, commonly used when the number of classes or segments is known in advance 
            (or we wish to explore multiple possible segmentations). It groups pixels into clusters based on their feature vectors (e.g., intensity, color, texture).
        </p>
        <ul>
            <li>
                <strong>K-Means Clustering:</strong><br>
                Partitions data into \(k\) clusters by minimizing the within-cluster sum of squares. 
                Often used for simple color-based segmentation or to separate an image into regions of similar intensity.
            </li>
            <li>
                <strong>Gaussian Mixture Models (GMM):</strong><br>
                Models the data distribution with multiple Gaussian components. Each pixel is assigned to the component (cluster) that maximizes its posterior probability.
            </li>
        </ul>

        <!-- Code Tabs: Python and MATLAB (K-Means Example) -->
        <div class="code-tabs">
            <div class="code-tab active" onclick="switchCode('python-clustering')">Python</div>
            <div class="code-tab" onclick="switchCode('matlab-clustering')">MATLAB</div>
        </div>

        <div id="python-clustering" class="code-block active">
            <pre>
from sklearn.cluster import KMeans
import numpy as np

# Assume 'image' is a 2D grayscale array
pixels = image.reshape(-1, 1)  # Flatten to (N,1)

# K-Means with 3 clusters
kmeans = KMeans(n_clusters=3, random_state=42).fit(pixels)
labels = kmeans.labels_.reshape(image.shape)

# 'labels' now contains values 0,1,2 representing the cluster each pixel belongs to
            </pre>
        </div>

        <div id="matlab-clustering" class="code-block">
            <pre>
% Flatten the image into a column vector
pixels = double(image(:));

% Apply K-Means clustering with 3 clusters
n_clusters = 3;
[idx, ~] = kmeans(pixels, n_clusters);

% Reshape the clustered labels back to the original image dimensions
segmented_image = reshape(idx, size(image));
            </pre>
        </div>

        <!-- 3.5 DEEP LEARNING-BASED SEGMENTATION -->
        <h3>5. Deep Learning-Based Segmentation</h3>
        <p>
            With the advancement of deep neural networks, <strong>deep learning-based</strong> methods have dramatically improved segmentation accuracy and robustness, 
            often surpassing traditional approaches in challenging, real-world scenarios.
        </p>
        <ul>
            <li>
                <strong>U-Net:</strong><br>
                Originally designed for biomedical image segmentation. Its symmetrical encoder-decoder architecture learns both context and precise localization.
            </li>
            <li>
                <strong>Mask R-CNN:</strong><br>
                An extension of Faster R-CNN that performs instance segmentation. 
                It detects objects and predicts a binary mask for each instance, widely used in applications like autonomous driving and robotics.
            </li>
            <li>
                <strong>Fully Convolutional Networks (FCN):</strong><br>
                Replace the fully connected layers in standard CNNs with convolutional ones, enabling end-to-end pixel-wise classification.
            </li>
        </ul>
        <p>
            Although these methods typically require large labeled datasets and significant computational resources (often GPUs), 
            they are currently the state-of-the-art for many complex segmentation tasks.
        </p>

        <!-- 4. APPLICATIONS OF IMAGE SEGMENTATION -->
        <h2>Applications of Image Segmentation</h2>
        <p>
            Segmentation is essential across a multitude of domains:
        </p>
        <ul>
            <li>
                <strong>Medical Imaging:</strong><br>
                Automatic detection and delineation of tumors, organs, or lesions in modalities such as MRI, CT, and ultrasound.
            </li>
            <li>
                <strong>Autonomous Vehicles:</strong><br>
                Lane detection, pedestrian recognition, and obstacle avoidance rely on consistent segmentation in real-time.
            </li>
            <li>
                <strong>Satellite Imaging:</strong><br>
                Land cover classification, vegetation mapping, and urban development monitoring leverage segmentation of large-scale remote-sensing imagery.
            </li>
            <li>
                <strong>Security Systems:</strong><br>
                Face, fingerprint, or iris segmentation is critical for biometric authentication and identification.
            </li>
        </ul>

        <!-- 5. CHALLENGES AND CONSIDERATIONS -->
        <h2>Challenges and Considerations</h2>
        <p>
            Despite the variety of techniques, segmentation remains a challenging field:
        </p>
        <ul>
            <li>
                <strong>Varying Illumination and Contrast:</strong><br>
                Changes in lighting can significantly affect pixel intensities, complicating thresholding and region-based approaches.
            </li>
            <li>
                <strong>Complex Shapes and Textures:</strong><br>
                Real-world objects often lack uniform color or intensity; advanced methods or deep learning may be required to segment them accurately.
            </li>
            <li>
                <strong>High Computational Cost:</strong><br>
                Processing large or high-resolution images can be expensive, especially for sophisticated algorithms (e.g., watershed, 3D volumetric data).
            </li>
            <li>
                <strong>Choosing the Right Method:</strong><br>
                No single technique is universally best. The choice depends on the nature of the images, noise levels, domain requirements, and computational constraints.
            </li>
        </ul>

        <!-- 6. FURTHER LEARNING RESOURCES -->
        <h2>Further Learning Resources</h2>
        <p>
            For those interested in diving deeper into image segmentation, the following resources offer tutorials, libraries, and theoretical foundations:
        </p>
        <ul>
            <li><a href="https://opencv.org/">OpenCV - Computer vision library</a> – Provides ready-to-use segmentation functions.</li>
            <li><a href="https://scikit-image.org/">scikit-image - Image processing in Python</a> – Includes thresholding, region growing, and more.</li>
            <li><a href="https://www.coursera.org/learn/digital-image-processing">Coursera - Digital Image Processing</a> – In-depth online courses covering traditional and modern segmentation methods.</li>
            <li><a href="https://en.wikipedia.org/wiki/Image_segmentation">Image Segmentation (Wikipedia)</a> – A high-level overview and links to research papers.</li>
            <li><em>Digital Image Processing</em> by Gonzalez & Woods – A classic textbook containing fundamental segmentation algorithms and theory.</li>
        </ul>

        <!-- 7. INTERACTIVE DEMOS -->
        <h2>Interactive Demos</h1>
        <p>Below are live demos illustrating some key segmentation methods—Thresholding, Region Growing, and basic K-Means Clustering—directly in the browser.</p>
          <!-- The source image used for segmentation demos -->
          <div style="display: none;">
            <img id="sourceSegmentImage" src="image1.jpeg" alt="Source Image" />
          </div>
        
          <!-- ============== DEMO 1: Threshold Segmentation ============== -->
          <div class="demo-toggle" onclick="toggleDemo('demo-threshold')">
            <strong>Threshold-Based Segmentation</strong>
          </div>
          <div id="demo-threshold" class="demo-content">
            <p>
              This demo shows how a simple global threshold can segment foreground vs. background. 
              Drag the slider to select a threshold (0-255). All pixels with an intensity above this value will be set to white (foreground), and the rest to black (background).
            </p>
            <div class="slider-group">
              <label for="thresholdValue">Threshold:</label>
              <input type="range" id="thresholdValue" min="0" max="255" value="128" />
              <span id="thresholdValueLabel">128</span>
            </div>
            <canvas id="canvasThreshold" width="600" height="400"></canvas>
            <div class="explanation">
              <strong>How it works:</strong> Each pixel is converted to grayscale by averaging its R, G, and B channels. 
              If grayscale_intensity &gt; threshold, we set that pixel to white; otherwise black.
            </div>
          </div>
        
          <!-- ============== DEMO 2: Region Growing ============== -->
          <div class="demo-toggle" onclick="toggleDemo('demo-region-growing')">
            <strong>Region Growing Segmentation</strong>
          </div>
          <div id="demo-region-growing" class="demo-content">
            <p>
              Click on the image to pick a "seed" pixel. The algorithm will expand outwards from that seed, adding neighboring pixels that are within a specified intensity difference. 
              Use the slider to adjust the tolerance.
            </p>
            <div class="slider-group">
              <label for="regionTolerance">Tolerance:</label>
              <input type="range" id="regionTolerance" min="1" max="100" value="20" />
              <span id="regionToleranceLabel">20</span>
            </div>
            <canvas id="canvasRegionGrowing" width="600" height="400"></canvas>
            <div class="explanation">
              <strong>How it works:</strong> When you click on the canvas, we get the seed pixel’s intensity. We then do a 
              flood-fill (like BFS or DFS) outwards from that seed, including neighboring pixels whose intensity differs 
              by less than <em>tolerance</em> from the seed. Those pixels get marked in a distinct color (e.g., red).
            </div>
          </div>
        
          <!-- ============== DEMO 3: Basic K-Means Clustering ============== -->
          <div class="demo-toggle" onclick="toggleDemo('demo-kmeans')">
            <strong>K-Means Segmentation</strong>
          </div>
          <div id="demo-kmeans" class="demo-content">
            <p>
              The image is segmented into <strong>k</strong> clusters based on color similarity (in RGB space). Use the slider to change the 
              number of clusters and see how the regions change.
            </p>
            <div class="slider-group">
              <label for="kValue">Number of Clusters (k):</label>
              <input type="range" id="kValue" min="2" max="8" value="3" />
              <span id="kValueLabel">3</span>
            </div>
            <canvas id="canvasKmeans" width="600" height="400"></canvas>
            <div class="explanation">
              <strong>How it works:</strong> We extract each pixel’s (R,G,B) values as a 3D point. K-means tries to find 
              <em>k</em> centroids in this color space. Each pixel is assigned to the nearest centroid, and we replace 
              the pixel’s color with the centroid color, resulting in <em>k</em> distinct color regions.
            </div>  
        </div>
        <div class="footer">
          <p>&copy; 2025 <a href="https://personal.us.es/amirabella/">A. Giuliano Mirabella</a></p>
      </div>
    </div>

    <script src="script.js"></script>
      <script>
        /**************************************************************
         * 1) ON PAGE LOAD: INITIALIZE ALL DEMOS
         **************************************************************/
        const sourceImg = document.getElementById('sourceSegmentImage');
        sourceImg.onload = function() {
          // Once the image is loaded, run initialization code for each demo.
          initThresholdDemo();
          initRegionGrowingDemo();
          initKmeansDemo();
        };

        if (sourceImg.complete) {
          sourceImg.onload();
        }

        /**************************************************************
         * 2) THRESHOLD SEGMENTATION DEMO
         *    - We convert each pixel to grayscale.
         *    - Compare grayscale value to the slider's threshold.
         *    - If above threshold => white; else => black.
         **************************************************************/
        function initThresholdDemo() {
          const thresholdCanvas = document.getElementById('canvasThreshold');
          const thresholdCtx = thresholdCanvas.getContext('2d');
          thresholdCanvas.width = sourceImg.width;
          thresholdCanvas.height = sourceImg.height;
    
          // Draw the source image initially
          thresholdCtx.drawImage(sourceImg, 0, 0, thresholdCanvas.width, thresholdCanvas.height);
    
          // Grab UI elements
          const thresholdSlider = document.getElementById('thresholdValue');
          const thresholdLabel = document.getElementById('thresholdValueLabel');
    
          // Add event listener to update the threshold
          thresholdSlider.addEventListener('input', () => {
            thresholdLabel.textContent = thresholdSlider.value;
            applyThreshold(thresholdSlider.value);
          });
    
          // Apply threshold to the image
          function applyThreshold(thValue) {
            // Get current image data (originally drawn or last updated)
            const imageData = thresholdCtx.getImageData(0, 0, thresholdCanvas.width, thresholdCanvas.height);
            const data = imageData.data; // array of [r, g, b, a, r, g, b, a, ...]
    
            // Process each pixel
            for (let i = 0; i < data.length; i += 4) {
              const r = data[i];
              const g = data[i+1];
              const b = data[i+2];
              // Convert to grayscale (simple average or luminosity method)
              const gray = Math.round((r + g + b) / 3);
    
              // Compare to threshold
              if (gray > thValue) {
                data[i] = 255;   // r
                data[i+1] = 255; // g
                data[i+2] = 255; // b
              } else {
                data[i] = 0;
                data[i+1] = 0;
                data[i+2] = 0;
              }
            }
    
            // Update the canvas
            thresholdCtx.putImageData(imageData, 0, 0);
          }
    
          // Initialize with default threshold
          applyThreshold(thresholdSlider.value);
        }
    
        /**************************************************************
         * 3) REGION GROWING DEMO
         *    - Convert image to grayscale.
         *    - On click, pick the seed pixel’s intensity.
         *    - Perform a flood-fill using BFS or DFS with "tolerance".
         **************************************************************/
        function initRegionGrowingDemo() {
          const regionCanvas = document.getElementById('canvasRegionGrowing');
          const regionCtx = regionCanvas.getContext('2d');
          regionCanvas.width = sourceImg.width;
          regionCanvas.height = sourceImg.height;
    
          // Draw the original image onto the canvas
          regionCtx.drawImage(sourceImg, 0, 0, regionCanvas.width, regionCanvas.height);
    
          // We'll maintain a separate buffer of the grayscale data for quick access
          let grayscaleData = null;
          // Also store the original image data so we can redraw quickly
          const originalImageData = regionCtx.getImageData(0, 0, regionCanvas.width, regionCanvas.height);
    
          // Convert the image to grayscale and store it
          function computeGrayscale() {
            const tempData = regionCtx.getImageData(0, 0, regionCanvas.width, regionCanvas.height);
            const data = tempData.data;
            grayscaleData = new Uint8ClampedArray(data.length / 4); // 1 grayscale byte per pixel
    
            for (let i = 0, j = 0; i < data.length; i += 4, j++) {
              const r = data[i];
              const g = data[i+1];
              const b = data[i+2];
              // Simple average
              grayscaleData[j] = (r + g + b) / 3;
            }
          }
    
          computeGrayscale();
    
          // Tolerance slider
          const toleranceSlider = document.getElementById('regionTolerance');
          const toleranceLabel = document.getElementById('regionToleranceLabel');
          toleranceSlider.addEventListener('input', () => {
            toleranceLabel.textContent = toleranceSlider.value;
          });
    
          // On canvas click, run region growing from the clicked pixel
          regionCanvas.addEventListener('click', (evt) => {
            const rect = regionCanvas.getBoundingClientRect();
            const x = Math.floor(evt.clientX - rect.left);
            const y = Math.floor(evt.clientY - rect.top);
    
            // Redraw the original image, so we remove any old region highlight
            regionCtx.putImageData(originalImageData, 0, 0);
    
            // Apply region growing
            const tol = parseInt(toleranceSlider.value, 10);
            growRegion(x, y, tol);
          });
    
          // BFS region growing
          function growRegion(seedX, seedY, tolerance) {
            // We use a queue to implement BFS
            const queue = [];
            const visited = new Uint8Array(grayscaleData.length); // track visited pixels (0 or 1)
    
            // Get seed intensity
            const seedIndex = seedY * regionCanvas.width + seedX;
            const seedIntensity = grayscaleData[seedIndex];
    
            // Start BFS
            queue.push({x: seedX, y: seedY});
            visited[seedIndex] = 1;
    
            // We'll manipulate a new ImageData for the final result
            const newImageData = regionCtx.getImageData(0, 0, regionCanvas.width, regionCanvas.height);
            const outputData = newImageData.data;
    
            while (queue.length > 0) {
              const {x, y} = queue.shift();
              // Mark the pixel in red to highlight the region
              const idx = (y * regionCanvas.width + x) * 4;
              outputData[idx] = 255;   // R
              outputData[idx+1] = 0;   // G
              outputData[idx+2] = 0;   // B
              outputData[idx+3] = 255; // A
    
              // Check neighbors (4-connectivity: up, down, left, right)
              const neighbors = [
                {nx: x,   ny: y-1},
                {nx: x,   ny: y+1},
                {nx: x-1, ny: y},
                {nx: x+1, ny: y}
              ];
              for (let n of neighbors) {
                if (
                  n.nx >= 0 && n.nx < regionCanvas.width &&
                  n.ny >= 0 && n.ny < regionCanvas.height
                ) {
                  const nIdx = n.ny * regionCanvas.width + n.nx;
                  if (!visited[nIdx]) {
                    // Check intensity difference
                    const intensityDiff = Math.abs(grayscaleData[nIdx] - seedIntensity);
                    if (intensityDiff <= tolerance) {
                      visited[nIdx] = 1;
                      queue.push({x: n.nx, y: n.ny});
                    }
                  }
                }
              }
            }
    
            // Draw the updated region
            regionCtx.putImageData(newImageData, 0, 0);
          }
        }
    
        /**************************************************************
         * 4) K-MEANS CLUSTERING DEMO
         *    - Convert each pixel’s (R,G,B) into a 3D point
         *    - Perform K-means
         *    - Replace pixel colors with their cluster centroid color
         **************************************************************/
        function initKmeansDemo() {
          const kCanvas = document.getElementById('canvasKmeans');
          const kCtx = kCanvas.getContext('2d');
          kCanvas.width = sourceImg.width;
          kCanvas.height = sourceImg.height;
        
          // 1) Draw the source image once
          kCtx.drawImage(sourceImg, 0, 0, kCanvas.width, kCanvas.height);
        
          // 2) Grab the original image data and store it
          const originalImgData = kCtx.getImageData(0, 0, kCanvas.width, kCanvas.height);
        
          // Slider input
          const kSlider = document.getElementById('kValue');
          const kLabel = document.getElementById('kValueLabel');
        
          // Listen for slider changes
          kSlider.addEventListener('input', () => {
            kLabel.textContent = kSlider.value;
            applyKmeans(parseInt(kSlider.value, 10));
          });
        
          // Run once at the default k
          applyKmeans(parseInt(kSlider.value, 10));
        
          function applyKmeans(k) {
            // Always restore the original image first
            kCtx.putImageData(originalImgData, 0, 0);
        
            // Now get the fresh (non-segmented) data for clustering
            let imageData = kCtx.getImageData(0, 0, kCanvas.width, kCanvas.height);
            let data = imageData.data; // [r, g, b, a, ...]
        
            // Build an array of color points
            let points = [];
            for (let i = 0; i < data.length; i += 4) {
              points.push({r: data[i], g: data[i+1], b: data[i+2]});
            }
        
            // 3) Perform K-means from scratch using these points
            // (Re-randomizing centroids, re-assigning points, etc.)
            let clusterAssignments = runKmeans(points, k);
        
            // 4) Update the canvas data based on the final cluster assignments
            for (let i = 0; i < points.length; i++) {
              const cIdx = clusterAssignments[i].cluster;
              data[4*i]   = cIdx.r; // replace with centroid color
              data[4*i+1] = cIdx.g;
              data[4*i+2] = cIdx.b;
              // alpha remains the same or set to 255
            }
        
            // 5) Draw the updated result to the canvas
            kCtx.putImageData(imageData, 0, 0);
          }
        
          // Example k-means function returning an array { cluster: {r, g, b} } per pixel
          function runKmeans(points, k) {
            // Initialize centroids randomly
            let centroids = initializeCentroids(points, k);
            let assignments = new Array(points.length);
        
            let changed = true;
            let maxIter = 10;
            let iter = 0;
        
            while (changed && iter < maxIter) {
              changed = false;
              iter++;
        
              // 1) Assign each point to the closest centroid
              for (let i = 0; i < points.length; i++) {
                let bestDist = Infinity;
                let bestC = 0;
                for (let c = 0; c < centroids.length; c++) {
                  let dist = colorDistance(points[i], centroids[c]);
                  if (dist < bestDist) {
                    bestDist = dist;
                    bestC = c;
                  }
                }
                if (!assignments[i] || assignments[i].centroidIndex !== bestC) {
                  assignments[i] = { centroidIndex: bestC };
                  changed = true;
                }
              }
        
              // 2) Recompute centroids
              let sums = new Array(k).fill(null).map(() => ({r:0,g:0,b:0,count:0}));
              for (let i = 0; i < points.length; i++) {
                let cIdx = assignments[i].centroidIndex;
                sums[cIdx].r += points[i].r;
                sums[cIdx].g += points[i].g;
                sums[cIdx].b += points[i].b;
                sums[cIdx].count++;
              }
              for (let c = 0; c < k; c++) {
                if (sums[c].count > 0) {
                  centroids[c].r = sums[c].r / sums[c].count;
                  centroids[c].g = sums[c].g / sums[c].count;
                  centroids[c].b = sums[c].b / sums[c].count;
                }
              }
            }
        
            // Return final cluster color for each pixel
            return assignments.map(a => ({
              cluster: {
                r: centroids[a.centroidIndex].r,
                g: centroids[a.centroidIndex].g,
                b: centroids[a.centroidIndex].b
              }
            }));
          }
        
          function initializeCentroids(points, k) {
            let centroids = [];
            for (let c = 0; c < k; c++) {
              let randIdx = Math.floor(Math.random() * points.length);
              centroids.push({
                r: points[randIdx].r,
                g: points[randIdx].g,
                b: points[randIdx].b
              });
            }
            return centroids;
          }
        
          function colorDistance(a, b) {
            const dr = a.r - b.r;
            const dg = a.g - b.g;
            const db = a.b - b.b;
            return dr*dr + dg*dg + db*db;
          }
        }
    </script>
</body>
</html>
