<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <meta name="description" content="Feature Extraction and Image Descriptors">
    <meta name="author" content="A. Giuliano Mirabella">
    <title>Feature Extraction and Image Descriptors</title>
    <link rel="stylesheet" href="styles.css">
</head>
<body>
    <header>
        <h1>Feature Extraction and Image Descriptors</h1>
        <p>Understanding key techniques for image feature representation, from fundamental concepts to advanced methods</p>
    </header>

    <div class="container">
        <!-- 1. WHAT IS FEATURE EXTRACTION? -->
        <h2>What is Feature Extraction?</h2>
        <p>
            <strong>Feature extraction</strong> is the process of detecting, describing, and representing salient characteristics of an image 
            in a numerical or symbolic form. The goal is to reduce the dimensionality of the original image data 
            (often consisting of millions of raw pixels) into a more compact and discriminative representation. 
            These features may capture color, shape, texture, or other domain-specific attributes that are relevant 
            for tasks like classification, object recognition, registration, and image retrieval.
        </p>
        <p>
            In essence, feature extraction translates raw pixel intensities into <em>descriptors</em> or <em>signatures</em> 
            that can be more easily interpreted or compared by machine learning algorithms, 
            thereby facilitating tasks such as image matching, pattern analysis, and decision-making.
        </p>

        <!-- 2. IMPORTANCE OF FEATURE EXTRACTION -->
        <h2>Importance of Feature Extraction</h2>
        <p>
            Feature extraction plays a critical role in computer vision and image processing because it:
        </p>
        <ul>
            <li>
                <strong>Reduces Data Complexity:</strong><br>
                By focusing on the most relevant aspects (e.g., edges, contours, texture statistics), 
                we ignore redundant pixel-level information, leading to faster and more memory-efficient processing.
            </li>
            <li>
                <strong>Improves Performance:</strong><br>
                High-quality features can significantly boost the accuracy and robustness of machine learning or deep learning models. 
                Good features often determine the difference between success and failure in tasks like object detection or medical diagnosis.
            </li>
            <li>
                <strong>Facilitates Comparisons:</strong><br>
                Consistent descriptors (e.g., local keypoints) enable matching of the same object or pattern across different viewpoints, scales, or lighting conditions, 
                thereby supporting tasks like image alignment, scene stitching, and recognition.
            </li>
        </ul>

        <!-- 3. TYPES OF FEATURES -->
        <h2>Types of Features</h2>
        <p>
            Although features can vary in their mathematical formulation, they are often conceptually grouped as:
        </p>
        <ul>
            <li>
                <strong>Global Features:</strong><br>
                Describe the entire image holistically. Examples include <em>color histograms</em>, 
                <em>global texture descriptors</em> (e.g., overall frequency content), or <em>moments</em> capturing statistical information about pixel intensities.
            </li>
            <li>
                <strong>Local Features:</strong><br>
                Characterize small regions or keypoints within the image. These features can capture fine-grained details and are more robust to occlusion, illumination changes, 
                and other variations. Typical examples include <em>SIFT keypoints</em>, <em>ORB descriptors</em>, and <em>corner detectors</em>.
            </li>
        </ul>

        <!-- 4. COMMON FEATURE EXTRACTION TECHNIQUES -->
        <h2>Common Feature Extraction Techniques</h2>
        <p>
            A wide array of feature extraction methods exist, each suited for different applications and image conditions. 
            Below are some of the most commonly used techniques, along with brief code snippets in Python and MATLAB.
        </p>

        <!-- 4.1 EDGE DETECTION -->
        <h3>1. Edge Detection</h3>
        <p>
            <strong>Edge detection</strong> highlights the boundaries of objects by capturing sharp changes or discontinuities in intensity. 
            It is often an important first step in further shape or contour analysis.
        </p>
        <ul>
            <li>
                <strong>Sobel Operator:</strong><br>
                Computes the gradient magnitude and direction in orthogonal directions (typically x and y). 
                A fairly straightforward approach that is useful for quick edge detection and is computationally inexpensive.
            </li>
            <li>
                <strong>Canny Edge Detector:</strong><br>
                A multi-stage algorithm incorporating noise reduction, gradient calculation, non-maximum suppression, and hysteresis thresholding. 
                Known for its robustness in detecting true edges while minimizing false positives.
            </li>
        </ul>

        <div class="code-tabs">
            <div class="code-tab active" onclick="switchCode('python-edge')">Python</div>
            <div class="code-tab" onclick="switchCode('matlab-edge')">MATLAB</div>
        </div>

        <div id="python-edge" class="code-block active">
            <pre>
import cv2

# Load image in grayscale
image = cv2.imread('image.jpg', cv2.IMREAD_GRAYSCALE)

# Apply Canny edge detection
edges = cv2.Canny(image, 50, 150)
cv2.imwrite('edges.jpg', edges)
            </pre>
        </div>

        <div id="matlab-edge" class="code-block">
            <pre>
% Edge detection
I = imread('image.jpg');
Igray = rgb2gray(I);
edges = edge(Igray, 'Canny', [0.1 0.3]);
imwrite(edges, 'edges.jpg');
            </pre>
        </div>

        <!-- 4.2 TEXTURE ANALYSIS -->
        <h3>2. Texture Analysis</h3>
        <p>
            <strong>Texture features</strong> aim to characterize the repetitive or quasi-repetitive patterns in an image. 
            They are particularly useful in applications like material classification, medical diagnostics (e.g., detecting abnormalities in tissue textures), and face recognition.
        </p>
        <ul>
            <li>
                <strong>Gray Level Co-occurrence Matrix (GLCM):</strong><br>
                Captures the frequency of co-occurrences of intensity values at specified spatial relationships (distance and angle). 
                Statistical measures derived from the GLCM (e.g., contrast, correlation, energy, homogeneity) serve as texture descriptors.
            </li>
            <li>
                <strong>Local Binary Patterns (LBP):</strong><br>
                Compares each pixel to its neighbors in a local region, encoding the pattern of intensity differences as a binary string. 
                LBP is rotation-invariant and computationally efficient, making it popular for texture classification.
            </li>
        </ul>

        <div class="code-tabs">
            <div class="code-tab active" onclick="switchCode('python-texture')">Python</div>
            <div class="code-tab" onclick="switchCode('matlab-texture')">MATLAB</div>
        </div>

        <div id="python-texture" class="code-block active">
            <pre>
from skimage.feature import graycomatrix, graycoprops
import cv2

# Load or define a grayscale image
image = cv2.imread('image.jpg', cv2.IMREAD_GRAYSCALE)

# Compute GLCM
glcm = graycomatrix(image, distances=[1], angles=[0], levels=256,
                    symmetric=True, normed=True)
contrast = graycoprops(glcm, 'contrast')[0, 0]
print("Contrast:", contrast)
            </pre>
        </div>

        <div id="matlab-texture" class="code-block">
            <pre>
% Compute GLCM in MATLAB
I = imread('image.jpg');
Igray = rgb2gray(I);
offsets = [0 1];  % Horizontal neighbor
glcm = graycomatrix(Igray, 'Offset', offsets, 'Symmetric', true);
stats = graycoprops(glcm, 'Contrast');
contrast = stats.Contrast;
disp(['Contrast: ', num2str(contrast)]);
            </pre>
        </div>

        <!-- 4.3 SHAPE DESCRIPTORS -->
        <h3>3. Shape Descriptors</h3>
        <p>
            <strong>Shape-based features</strong> describe the geometric properties of objects in an image. 
            They often rely on boundary or contour extraction, enabling tasks such as object classification by shape, pose estimation, or defect detection.
        </p>
        <ul>
            <li>
                <strong>Contours:</strong><br>
                A sequence of points forming the boundary of an object. 
                Extracting contours is typically done on a binarized or segmented image.
            </li>
            <li>
                <strong>Hu Moments:</strong><br>
                Seven moment invariants (originally introduced by Hu) that remain relatively constant under translation, rotation, and scaling. 
                Useful for shape recognition when objects have consistent contours but may appear at different scales or orientations.
            </li>
        </ul>

        <div class="code-tabs">
            <div class="code-tab active" onclick="switchCode('python-shape')">Python</div>
            <div class="code-tab" onclick="switchCode('matlab-shape')">MATLAB</div>
        </div>

        <div id="python-shape" class="code-block active">
            <pre>
import cv2
import numpy as np

# Load binary or thresholded image
image = cv2.imread('image.jpg', cv2.IMREAD_GRAYSCALE)
# Find contours
contours, hierarchy = cv2.findContours(image, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)

# Draw contours on a color copy
color_image = cv2.cvtColor(image, cv2.COLOR_GRAY2BGR)
cv2.drawContours(color_image, contours, -1, (0, 255, 0), 2)
cv2.imwrite('contours.jpg', color_image)

# Compute Hu Moments for the first contour (example)
if contours:
    moments = cv2.moments(contours[0])
    huMoments = cv2.HuMoments(moments)
    print("Hu Moments for first contour:", huMoments.flatten())
            </pre>
        </div>

        <div id="matlab-shape" class="code-block">
            <pre>
% Find contours and compute shape descriptors in MATLAB
I = imread('image.jpg');
Igray = rgb2gray(I);
BW = imbinarize(Igray);
BW = imfill(BW, 'holes');

% Extract boundaries
boundaries = bwboundaries(BW, 'noholes');
Ioverlay = I;

for k = 1:length(boundaries)
    boundary = boundaries{k};
    for n = 1:size(boundary, 1)
        row = boundary(n,1);
        col = boundary(n,2);
        Ioverlay(row, col, :) = [0, 255, 0]; % Draw boundary in green
    end
    
    % Compute moments (using regionprops)
    stats = regionprops(BW, 'Moments');
    % (regionprops can also give Hu moments if needed)
end

imwrite(Ioverlay, 'contours.jpg');
            </pre>
        </div>

        <!-- 4.4 KEYPOINT DETECTION AND MATCHING -->
        <h3>4. Keypoint Detection and Matching</h3>
        <p>
            <strong>Keypoints</strong> (a.k.a. interest points or salient points) are well-defined, highly distinctive points in an image. 
            They form the basis for many image alignment, stitching, and recognition algorithms, 
            as each keypoint can be associated with a local <em>descriptor</em> that is invariant (or robust) to transformations such as rotation or scaling.
        </p>
        <ul>
            <li>
                <strong>SIFT (Scale-Invariant Feature Transform):</strong><br>
                A pioneering algorithm that detects keypoints at multiple scales and generates robust descriptors based on local gradient distributions.
            </li>
            <li>
                <strong>SURF (Speeded-Up Robust Features):</strong><br>
                A more computationally efficient variant of SIFT that uses integral images and approximations of Gaussian derivatives.
            </li>
            <li>
                <strong>ORB (Oriented FAST and Rotated BRIEF):</strong><br>
                An open-source-friendly algorithm (not encumbered by patents like SIFT/SURF). 
                Combines the FAST keypoint detector with the BRIEF descriptor, with orientation compensation to handle in-plane rotations.
            </li>
        </ul>

        <div class="code-tabs">
            <div class="code-tab active" onclick="switchCode('python-keypoint')">Python</div>
            <div class="code-tab" onclick="switchCode('matlab-keypoint')">MATLAB</div>
        </div>

        <div id="python-keypoint" class="code-block active">
            <pre>
import cv2

# ORB keypoint detection
image = cv2.imread('image.jpg', cv2.IMREAD_GRAYSCALE)
orb = cv2.ORB_create()
keypoints, descriptors = orb.detectAndCompute(image, None)
output = cv2.drawKeypoints(image, keypoints, None, color=(0,255,0))
cv2.imwrite('keypoints.jpg', output)
            </pre>
        </div>

        <div id="matlab-keypoint" class="code-block">
            <pre>
% ORB keypoint detection in MATLAB
I = imread('image.jpg');
Igray = rgb2gray(I);

points = detectORBFeatures(Igray);
[features, validPoints] = extractFeatures(Igray, points);

% Create an output image showing keypoints
output = insertMarker(I, validPoints.Location, 'circle', 'Color', 'red');
imwrite(output, 'keypoints.jpg');
            </pre>
        </div>

        <!-- 4.5 COLOR DESCRIPTORS -->
        <h3>5. Color Descriptors</h3>
        <p>
            <strong>Color-based features</strong> quantify the distribution and relationships of colors within an image. 
            They play an essential role in applications like content-based image retrieval (CBIR), scene understanding, and object detection in color-based contexts.
        </p>
        <ul>
            <li>
                <strong>Color Histograms:</strong><br>
                Summarize the frequency of each color or intensity level. 
                This global descriptor can be computed efficiently and is rotation- and scale-invariant, but sensitive to large illumination changes.
            </li>
            <li>
                <strong>Color Moments:</strong><br>
                Measures (e.g., mean, variance, skewness) that provide a compact representation of the color distribution. 
                Particularly useful when a histogram-based approach might be too coarse or high-dimensional.
            </li>
        </ul>

        <div class="code-tabs">
            <div class="code-tab active" onclick="switchCode('python-color')">Python</div>
            <div class="code-tab" onclick="switchCode('matlab-color')">MATLAB</div>
        </div>

        <div id="python-color" class="code-block active">
            <pre>
import cv2
import numpy as np

image = cv2.imread('image.jpg')  # in BGR by default

# Compute grayscale histogram (for demonstration)
hist = cv2.calcHist([cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)], [0], None, [256], [0,256])
cv2.normalize(hist, hist, alpha=0, beta=1, norm_type=cv2.NORM_MINMAX)
print("Normalized Grayscale Histogram:", hist.flatten())
            </pre>
        </div>

        <div id="matlab-color" class="code-block">
            <pre>
% Compute grayscale histogram in MATLAB
I = imread('image.jpg');
if size(I,3) == 3
    I = rgb2gray(I);
end

counts = imhist(I, 256);
counts = counts / sum(counts);  % Normalize the histogram
disp('Normalized Grayscale Histogram:');
disp(counts');
            </pre>
        </div>

        <!-- 5. APPLICATIONS OF FEATURE EXTRACTION -->
        <h2>Applications of Feature Extraction</h2>
        <p>
            Feature extraction underpins a wide variety of real-world image processing and computer vision applications:
        </p>
        <ul>
            <li>
                <strong>Object Recognition:</strong><br>
                Classifying or identifying specific objects in images (e.g., faces, vehicles, landmarks) by matching extracted features to known models or reference data.
            </li>
            <li>
                <strong>Medical Imaging:</strong><br>
                Detecting subtle patterns or abnormalities (tumors, lesions, microcalcifications) through texture features or shape descriptors in radiological scans.
            </li>
            <li>
                <strong>Face Recognition:</strong><br>
                Facial landmark detection (e.g., eyes, nose, mouth) and descriptor computation to enable person identification in security or social media applications.
            </li>
            <li>
                <strong>Image Retrieval:</strong><br>
                Searching large databases by comparing extracted features (color histograms, keypoints) with a query image, a technique known as Content-Based Image Retrieval (CBIR).
            </li>
        </ul>

        <!-- 6. CHALLENGES AND CONSIDERATIONS -->
        <h2>Challenges and Considerations</h2>
        <p>
            Although feature extraction can greatly simplify complex image data, several hurdles remain:
        </p>
        <ul>
            <li>
                <strong>Invariance Issues:</strong><br>
                Features should be robust to variations in scale, rotation, viewpoint, and illumination. Designing or choosing features that remain consistent under all relevant transformations is challenging.
            </li>
            <li>
                <strong>Computational Efficiency:</strong><br>
                Some feature extraction algorithms (e.g., SIFT) can be computationally expensive for large datasets or real-time applications. 
                More efficient alternatives like ORB or hardware acceleration may be necessary.
            </li>
            <li>
                <strong>High-Dimensional Data:</strong><br>
                Complex descriptors (especially those involving multiple channels or large patch sizes) can explode in dimensionality, requiring dimensionality reduction or careful regularization.
            </li>
            <li>
                <strong>Task-Dependent Suitability:</strong><br>
                The “best” feature descriptor can vary significantly depending on the image domain (natural scenes vs. medical images), 
                data characteristics (color vs. grayscale), and final application (e.g., retrieval vs. classification).
            </li>
        </ul>

        <!-- 7. FURTHER LEARNING RESOURCES -->
        <h2>Further Learning Resources</h2>
        <p>
            For a deeper exploration of feature extraction methods and best practices, consult the following references:
        </p>
        <ul>
            <li><a href="https://opencv.org/">OpenCV - Computer vision library</a> – Offers a comprehensive set of feature detectors and descriptors (SIFT, SURF, ORB, etc.).</li>
            <li><a href="https://scikit-image.org/">scikit-image - Image processing in Python</a> – Includes GLCM, LBP, and other classical methods in a Pythonic API.</li>
            <li><a href="https://en.wikipedia.org/wiki/Feature_extraction">Feature Extraction (Wikipedia)</a> – High-level overview and links to related research articles.</li>
            <li><a href="https://www.kaggle.com/datasets">Kaggle - Image Processing Datasets</a> – Practice real-world feature extraction and classification tasks on open datasets.</li>
            <li><em>Digital Image Processing</em> by Gonzalez & Woods – A classic textbook that covers foundational feature extraction theories and algorithms.</li>
        </ul>
        
        <!-- 8. INTERACTIVE DEMOS -->
        <h2>Interactive Demos</h2>

        <!-- 1) LBP DEMO TOGGLE -->
        <div class="demo-toggle" onclick="toggleDemo('demo-lbp')">
          <strong>Local Binary Patterns (LBP)</strong>
        </div>
        <div id="demo-lbp" class="demo-content">
          <p>
            Local Binary Patterns (LBP) is a simple yet efficient texture descriptor. 
            For each pixel, it looks at the 3×3 neighborhood, compares each neighbor's intensity to the center pixel, and forms an 8-bit binary code. 
            We then convert that code to a decimal value, which can be visualized (often used for texture analysis).
          </p>
    
          <!-- Controls for LBP Demo -->
          <div class="slider-group">
            <input type="checkbox" id="applyLBP" checked>
            <label for="applyLBP">Apply LBP</label>
          </div>
          <div class="slider-group">
            <label for="lbpRadius">Neighborhood Radius:</label>
            <input type="range" id="lbpRadius" min="1" max="3" value="1" step="1">
            <span id="lbpRadiusValue">1</span>
            <br><small>(We keep it small for simplicity; typical LBP uses a 3×3 or 3×3 ring.)</small>
          </div>
          <canvas id="canvasLBP"></canvas>
    
          <div class="explanation">
            <strong>How it works:</strong><br>
            For each pixel (x, y), we collect the intensities of its neighbors in a circular or square neighborhood (here, a 3×3 block for radius=1). 
            Each neighbor that is >= the center pixel is assigned a bit value of 1; otherwise, 0. 
            These bits form an 8-bit number, which we map to a grayscale value to display the LBP pattern.
          </div>
        </div>
    
        <!-- 2) COLOR HISTOGRAM DEMO TOGGLE -->
        <div class="demo-toggle" onclick="toggleDemo('demo-hist')">
          <strong>Color Histogram</strong>
        </div>
        <div id="demo-hist" class="demo-content">
          <p>
            A color histogram is one of the simplest global descriptors. It counts how often each color value appears. 
            Below, we compute a histogram for each channel (Red, Green, Blue) and display it as a simple bar chart.
          </p>
    
          <!-- Controls for histogram Demo -->
          <div class="slider-group">
            <label for="histBins">Number of bins:</label>
            <input type="range" id="histBins" min="4" max="64" value="16" step="4">
            <span id="histBinsValue">16</span>
            <br><small>(Adjust to see more/less resolution in the histogram)</small>
          </div>
    
          <!-- We'll display the original image + the histogram chart side by side or stacked -->
          <canvas id="canvasHistImage"></canvas>
          <canvas id="canvasHistogram" width="300" height="200"></canvas>
    
          <div class="explanation">
            <strong>How it works:</strong><br>
            We read the pixel values from the original image. For each channel (R, G, B), we increment the appropriate bin index based on the pixel’s intensity. 
            Then we normalize the counts and draw a bar chart for each channel in a different color.
          </div>
        </div>
    
        <!-- Source image for demos (hidden) -->
        <img id="sourceImage" src="image1.jpeg" alt="Demo Image" style="display:none;">
    </div>

    <footer>
        <p>&copy; 2025 <a href="https://personal.us.es/amirabella/">A. Giuliano Mirabella</a></p>
    </footer>
    <script src="script.js"></script>
    <script>
        /************************************************************************
         * 1) GENERAL SETUP: LOADING THE IMAGE & TOGGLE
         ************************************************************************/
        const sourceImg = document.getElementById('sourceImage');
        
        // Wait until the image is fully loaded before initializing
        sourceImg.onload = function() {
          initLBPDemo();
          initColorHistDemo();
        };

        if (sourceImg.complete) {
          sourceImg.onload();
        }

        // Simple toggle function to expand/collapse each demo content
        function toggleDemo(id) {
          const el = document.getElementById(id);
          if (!el) return;
          el.style.display = (el.style.display === 'block') ? 'none' : 'block';
        }
    
        /************************************************************************
         * 2) LBP DEMO
         ************************************************************************/
        function initLBPDemo() {
          const applyLBPCheckbox = document.getElementById('applyLBP');
          const lbpRadiusSlider  = document.getElementById('lbpRadius');
          const lbpRadiusValue   = document.getElementById('lbpRadiusValue');
          const canvasLBP        = document.getElementById('canvasLBP');
          const ctxLBP           = canvasLBP.getContext('2d');
    
          // Match canvas size to the source image
          canvasLBP.width  = sourceImg.width;
          canvasLBP.height = sourceImg.height;
    
          // Draw the source image first
          ctxLBP.drawImage(sourceImg, 0, 0);
          let originalImageData = ctxLBP.getImageData(0, 0, canvasLBP.width, canvasLBP.height);
    
          // Listen for changes
          applyLBPCheckbox.addEventListener('change', updateLBP);
          lbpRadiusSlider.addEventListener('input', () => {
            lbpRadiusValue.textContent = lbpRadiusSlider.value;
            updateLBP();
          });
    
          // Initial render
          updateLBP();
    
          // This function re-applies the LBP (or shows original) whenever a control changes
          function updateLBP() {
            // Start from the original image each time
            ctxLBP.putImageData(originalImageData, 0, 0);
            
            if (!applyLBPCheckbox.checked) {
              return; // if LBP not checked, just display original
            }
    
            let radius = parseInt(lbpRadiusSlider.value, 10);
    
            // We'll read the pixels from the original for processing
            const width  = canvasLBP.width;
            const height = canvasLBP.height;
            let srcData  = originalImageData.data; // RGBA array
            // Create a new ImageData for the LBP result
            let outputData = ctxLBP.createImageData(width, height);
            
            // Helper to get pixel intensity (grayscale) from RGBA
            function getGrayValue(x, y) {
              let idx = (y * width + x) * 4;
              let r = srcData[idx], g = srcData[idx+1], b = srcData[idx+2];
              // Simple average or luminosity formula
              return 0.299*r + 0.587*g + 0.114*b;
            }
    
            // For each pixel, compute LBP
            for (let y = radius; y < height - radius; y++) {
              for (let x = radius; x < width - radius; x++) {
                let centerVal = getGrayValue(x, y);
                let lbpCode = 0;
                /*
                  Typically, for radius=1, we sample the immediate 8 neighbors in a 3x3 block:
                    (x-1, y-1) (x, y-1) (x+1, y-1)
                    (x-1, y)   (x, y)   (x+1, y)
                    (x-1, y+1) (x, y+1) (x+1, y+1)
                  We'll assign bits in a clockwise or anticlockwise order. 
                */
    
                // Example: top-left neighbor
                let neighborVal0 = getGrayValue(x - radius, y - radius);
                lbpCode |= (neighborVal0 >= centerVal ? 1 : 0) << 7;
    
                // top neighbor
                let neighborVal1 = getGrayValue(x, y - radius);
                lbpCode |= (neighborVal1 >= centerVal ? 1 : 0) << 6;
    
                // top-right neighbor
                let neighborVal2 = getGrayValue(x + radius, y - radius);
                lbpCode |= (neighborVal2 >= centerVal ? 1 : 0) << 5;
    
                // right neighbor
                let neighborVal3 = getGrayValue(x + radius, y);
                lbpCode |= (neighborVal3 >= centerVal ? 1 : 0) << 4;
    
                // bottom-right neighbor
                let neighborVal4 = getGrayValue(x + radius, y + radius);
                lbpCode |= (neighborVal4 >= centerVal ? 1 : 0) << 3;
    
                // bottom neighbor
                let neighborVal5 = getGrayValue(x, y + radius);
                lbpCode |= (neighborVal5 >= centerVal ? 1 : 0) << 2;
    
                // bottom-left neighbor
                let neighborVal6 = getGrayValue(x - radius, y + radius);
                lbpCode |= (neighborVal6 >= centerVal ? 1 : 0) << 1;
    
                // left neighbor
                let neighborVal7 = getGrayValue(x - radius, y);
                lbpCode |= (neighborVal7 >= centerVal ? 1 : 0) << 0;
    
                // The lbpCode is between 0 and 255 (8-bit)
                // We can just map that directly to a grayscale intensity for display
                let idx = (y * width + x) * 4;
                outputData.data[idx+0] = lbpCode; // R
                outputData.data[idx+1] = lbpCode; // G
                outputData.data[idx+2] = lbpCode; // B
                outputData.data[idx+3] = 255;     // A (fully opaque)
              }
            }
    
            // Put the LBP image back to the canvas
            ctxLBP.putImageData(outputData, 0, 0);
          }
        }
    
        /************************************************************************
         * 3) COLOR HISTOGRAM DEMO
         ************************************************************************/
        function initColorHistDemo() {
          const canvasImage = document.getElementById('canvasHistImage');
          const ctxImage    = canvasImage.getContext('2d');
    
          const canvasHist  = document.getElementById('canvasHistogram');
          const ctxHist     = canvasHist.getContext('2d');
    
          const histBinsSlider = document.getElementById('histBins');
          const histBinsValue  = document.getElementById('histBinsValue');
    
          // Match the image canvas size to source
          canvasImage.width  = sourceImg.width;
          canvasImage.height = sourceImg.height;
          // Draw the original
          ctxImage.drawImage(sourceImg, 0, 0);
    
          // Listen for slider changes
          histBinsSlider.addEventListener('input', () => {
            histBinsValue.textContent = histBinsSlider.value;
            updateHistogram();
          });
    
          // Initial
          histBinsValue.textContent = histBinsSlider.value;
          updateHistogram();
    
          function updateHistogram() {
            const bins = parseInt(histBinsSlider.value, 10);
    
            // Clear the histogram canvas
            ctxHist.clearRect(0, 0, canvasHist.width, canvasHist.height);
    
            // Get the pixel data from the displayed image
            let imgData = ctxImage.getImageData(0, 0, canvasImage.width, canvasImage.height);
            let data = imgData.data; // RGBA array
    
            // We'll create arrays for R, G, B channels
            let histR = new Array(bins).fill(0);
            let histG = new Array(bins).fill(0);
            let histB = new Array(bins).fill(0);
    
            // Helper to figure out bin index for a pixel value
            //   pixel_value is in [0, 255], we have 'bins' bins
            //   bin_index = floor( (pixel_value / 256) * bins )
            function getBinIndex(value) {
              // clamp to [0,255] just in case
              value = Math.max(0, Math.min(255, value));
              return Math.floor((value / 256) * bins);
            }
    
            // Fill in the hist arrays
            for (let i = 0; i < data.length; i += 4) {
              let r = data[i + 0];
              let g = data[i + 1];
              let b = data[i + 2];
              histR[getBinIndex(r)]++;
              histG[getBinIndex(g)]++;
              histB[getBinIndex(b)]++;
            }
    
            // Normalize so we can draw them with a consistent scale
            let maxCount = Math.max(
              ...histR, 
              ...histG, 
              ...histB
            );
    
            // We want to draw side-by-side bars for R, G, B in each bin
            // We'll do a simple approach: each bin is subdivided into three narrower columns
            const barWidth  = (canvasHist.width - 20) / bins;      // total space for each bin
            const halfHeight = canvasHist.height - 20;             // some margin
            const offsetX   = 10;  // left margin
            const offsetY   = canvasHist.height - 10; // bottom margin
    
            // Draw axes or bounding box
            ctxHist.strokeStyle = '#000';
            ctxHist.strokeRect(0.5, 0.5, canvasHist.width - 1, canvasHist.height - 1);
    
            for (let i = 0; i < bins; i++) {
              // Normalized heights (0 -> 1) 
              let rFrac = histR[i] / maxCount;
              let gFrac = histG[i] / maxCount;
              let bFrac = histB[i] / maxCount;
    
              let subBarWidth = barWidth / 3.0;
    
              // R bar
              let rHeight = rFrac * halfHeight;
              ctxHist.fillStyle = 'red';
              ctxHist.fillRect(
                offsetX + i*barWidth, 
                offsetY - rHeight, 
                subBarWidth, 
                rHeight
              );
    
              // G bar
              let gHeight = gFrac * halfHeight;
              ctxHist.fillStyle = 'green';
              ctxHist.fillRect(
                offsetX + i*barWidth + subBarWidth, 
                offsetY - gHeight, 
                subBarWidth, 
                gHeight
              );
    
              // B bar
              let bHeight = bFrac * halfHeight;
              ctxHist.fillStyle = 'blue';
              ctxHist.fillRect(
                offsetX + i*barWidth + 2*subBarWidth, 
                offsetY - bHeight, 
                subBarWidth, 
                bHeight
              );
            }
          }
        }
  </script>
</body>
</html>
